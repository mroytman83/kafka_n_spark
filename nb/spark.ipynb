{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57991378-a17a-4201-ac22-93fc48012463",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cmontufar@wisc.edu\n",
    "#mroytman@wisc.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5029390d-4405-4664-b70b-adaf2a69c0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a951b322-2ed9-4393-8039-f36b78d03718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "\n",
    "spark = (SparkSession.builder.appName(\"cs544\")\n",
    "         .config(\"spark.sql.shuffle.partitions\", 10)\n",
    "         .config(\"spark.ui.showConsoleProgress\", False)\n",
    "         .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.2')\n",
    "         .getOrCreate())\n",
    "\n",
    "df = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "    .option(\"subscribe\", \"stations-json\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "schema = \"station STRING, date DATE, degrees DOUBLE, raining INTEGER\"\n",
    "df = (df.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"value\")).select(\"value.*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "055b1945-938a-41fa-bd67-980ff43f92dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d09d79f-043a-41ab-8775-6c6009b81490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/29 04:03:56 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-2b923f39-0e13-421f-85d6-4f41c38827d3. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/04/29 04:03:56 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|     start|       end|measurements|               avg|               max|\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|      F|2000-01-01|2000-07-05|         187|48.947904439104626| 91.64854721269778|\n",
      "|      K|2000-01-01|2000-07-05|         187| 38.09047603886158| 77.81071006033444|\n",
      "|      I|2000-01-01|2000-07-05|         187|60.674067778352054|104.57891274754704|\n",
      "|      N|2000-01-01|2000-07-05|         187| 43.88774160321495| 89.89182400725942|\n",
      "|      E|2000-01-01|2000-07-05|         187| 53.13786560506357| 91.36728474057894|\n",
      "|      J|2000-01-01|2000-07-05|         187| 64.82885978951383|108.70011908893918|\n",
      "|      A|2000-01-01|2000-07-05|         187| 53.37385214312269| 94.80152869347043|\n",
      "|      H|2000-01-01|2000-07-05|         187| 43.98722763613677| 89.45958927261715|\n",
      "|      G|2000-01-01|2000-07-05|         187| 36.92056513839209| 85.54628856747566|\n",
      "|      M|2000-01-01|2000-07-05|         187| 54.75252121517139| 95.09892939491029|\n",
      "|      L|2000-01-01|2000-07-05|         187| 51.16771535935375|101.17625756479876|\n",
      "|      D|2000-01-01|2000-07-05|         187|58.014323696927804| 100.9251202118109|\n",
      "|      O|2000-01-01|2000-07-05|         187| 46.28154483357943| 84.91094930293617|\n",
      "|      C|2000-01-01|2000-07-05|         187| 47.37068525090953| 97.52604699316623|\n",
      "|      B|2000-01-01|2000-07-05|         187| 51.05459515024819|  95.9723252760807|\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/29 04:04:01 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 5000 milliseconds, but spent 5848 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|     start|       end|measurements|               avg|               max|\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|      K|2000-01-01|2000-07-08|         190|38.622090250739866| 77.81071006033444|\n",
      "|      F|2000-01-01|2000-07-08|         190| 49.41408457359274| 91.64854721269778|\n",
      "|      I|2000-01-01|2000-07-08|         190|61.390705179362534|110.13804397065772|\n",
      "|      N|2000-01-01|2000-07-08|         190| 44.19091826319916| 89.89182400725942|\n",
      "|      E|2000-01-01|2000-07-08|         190| 53.30842833349492| 91.36728474057894|\n",
      "|      J|2000-01-01|2000-07-08|         190| 65.43032717874223|110.28619025734498|\n",
      "|      A|2000-01-01|2000-07-08|         190| 53.94121755821584| 94.80152869347043|\n",
      "|      H|2000-01-01|2000-07-08|         190| 44.58296140485102| 89.45958927261715|\n",
      "|      G|2000-01-01|2000-07-08|         190| 37.27449355536987| 85.54628856747566|\n",
      "|      M|2000-01-01|2000-07-08|         190|  55.0665087698615| 95.09892939491029|\n",
      "|      L|2000-01-01|2000-07-08|         190|52.005423079779746|107.26620838788999|\n",
      "|      D|2000-01-01|2000-07-08|         190|  58.3363105311977| 100.9251202118109|\n",
      "|      O|2000-01-01|2000-07-08|         190| 46.69413054198274| 84.91094930293617|\n",
      "|      C|2000-01-01|2000-07-08|         190|47.817493586006805| 97.52604699316623|\n",
      "|      B|2000-01-01|2000-07-08|         190| 51.48815689282122|  95.9723252760807|\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|     start|       end|measurements|               avg|               max|\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|      K|2000-01-01|2000-07-11|         193| 39.05281536275287| 77.81071006033444|\n",
      "|      F|2000-01-01|2000-07-11|         193| 50.06715999455527|  93.8527160399889|\n",
      "|      I|2000-01-01|2000-07-11|         193| 62.00131585536598|110.13804397065772|\n",
      "|      N|2000-01-01|2000-07-11|         193|44.493964391851804| 89.89182400725942|\n",
      "|      E|2000-01-01|2000-07-11|         193| 53.54948108422977| 91.36728474057894|\n",
      "|      J|2000-01-01|2000-07-11|         193| 65.81265656260533|110.28619025734498|\n",
      "|      A|2000-01-01|2000-07-11|         193| 54.57825400506406| 98.64620144006358|\n",
      "|      H|2000-01-01|2000-07-11|         193| 45.00925947688634| 89.45958927261715|\n",
      "|      G|2000-01-01|2000-07-11|         193|37.559903429548044| 85.54628856747566|\n",
      "|      M|2000-01-01|2000-07-11|         193|55.520313377586895| 95.09892939491029|\n",
      "|      L|2000-01-01|2000-07-11|         193| 52.60934112245915|107.26620838788999|\n",
      "|      D|2000-01-01|2000-07-11|         193| 58.52248371804319| 100.9251202118109|\n",
      "|      O|2000-01-01|2000-07-11|         193| 47.02981473898654| 84.91094930293617|\n",
      "|      C|2000-01-01|2000-07-11|         193| 48.22881061362246| 97.52604699316623|\n",
      "|      B|2000-01-01|2000-07-11|         193| 52.15157407688373| 97.51224357101201|\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|     start|       end|measurements|               avg|               max|\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|      K|2000-01-01|2000-07-16|         198| 39.56712242323931| 77.81071006033444|\n",
      "|      F|2000-01-01|2000-07-16|         198| 51.16719622109884| 96.70197910590964|\n",
      "|      I|2000-01-01|2000-07-16|         198|62.432546841860216|110.13804397065772|\n",
      "|      N|2000-01-01|2000-07-16|         198| 44.73643649744031| 89.89182400725942|\n",
      "|      E|2000-01-01|2000-07-16|         198| 54.44726401963616|101.18326393023928|\n",
      "|      J|2000-01-01|2000-07-16|         198| 66.67471754489715|110.28619025734498|\n",
      "|      A|2000-01-01|2000-07-16|         198|55.319133897105914| 98.64620144006358|\n",
      "|      H|2000-01-01|2000-07-16|         198| 46.13567187416385|  97.8769134273535|\n",
      "|      G|2000-01-01|2000-07-16|         198| 38.13607066719822| 85.54628856747566|\n",
      "|      M|2000-01-01|2000-07-16|         198| 56.19653239922918| 95.09892939491029|\n",
      "|      L|2000-01-01|2000-07-16|         198| 53.50206620278982|107.26620838788999|\n",
      "|      D|2000-01-01|2000-07-16|         198| 59.13480857881901| 100.9251202118109|\n",
      "|      O|2000-01-01|2000-07-16|         198| 47.83984171746592| 89.50249872154741|\n",
      "|      C|2000-01-01|2000-07-16|         198| 48.70091612936596| 97.52604699316623|\n",
      "|      B|2000-01-01|2000-07-16|         198| 52.70684912877961| 97.51224357101201|\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|     start|       end|measurements|               avg|               max|\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|      K|2000-01-01|2000-07-21|         203| 40.25835651744189| 78.38478611197569|\n",
      "|      F|2000-01-01|2000-07-21|         203| 52.15998425333905| 98.59218141448247|\n",
      "|      I|2000-01-01|2000-07-21|         203|62.912981507410265|110.13804397065772|\n",
      "|      N|2000-01-01|2000-07-21|         203| 44.79837036649242| 89.89182400725942|\n",
      "|      E|2000-01-01|2000-07-21|         203| 55.48451233612572|103.00216974026605|\n",
      "|      J|2000-01-01|2000-07-21|         203| 67.64151701938218|112.00708859014519|\n",
      "|      A|2000-01-01|2000-07-21|         203| 55.98125875333069| 98.64620144006358|\n",
      "|      H|2000-01-01|2000-07-21|         203| 46.95006436240904|  97.8769134273535|\n",
      "|      G|2000-01-01|2000-07-21|         203|38.602002059269964| 85.54628856747566|\n",
      "|      M|2000-01-01|2000-07-21|         203|56.733427747495654| 95.09892939491029|\n",
      "|      L|2000-01-01|2000-07-21|         203| 54.26305171983683|107.26620838788999|\n",
      "|      D|2000-01-01|2000-07-21|         203| 59.87984744810712| 100.9251202118109|\n",
      "|      O|2000-01-01|2000-07-21|         203| 48.89859290539872| 96.89470919142023|\n",
      "|      C|2000-01-01|2000-07-21|         203|49.097228777962975| 97.52604699316623|\n",
      "|      B|2000-01-01|2000-07-21|         203| 53.32777170463796| 97.51224357101201|\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|     start|       end|measurements|               avg|               max|\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|      K|2000-01-01|2000-07-26|         208| 41.14510253633526| 86.02396734508302|\n",
      "|      F|2000-01-01|2000-07-26|         208| 52.92119122423896| 98.59218141448247|\n",
      "|      I|2000-01-01|2000-07-26|         208| 63.37362253692839|110.13804397065772|\n",
      "|      N|2000-01-01|2000-07-26|         208|44.894120165486655| 89.89182400725942|\n",
      "|      E|2000-01-01|2000-07-26|         208| 56.07536440242632|103.00216974026605|\n",
      "|      J|2000-01-01|2000-07-26|         208| 68.46453110751818|112.00708859014519|\n",
      "|      A|2000-01-01|2000-07-26|         208| 56.85181083741648| 98.64620144006358|\n",
      "|      H|2000-01-01|2000-07-26|         208|47.427361924308734|  97.8769134273535|\n",
      "|      G|2000-01-01|2000-07-26|         208|39.085601736101246| 85.54628856747566|\n",
      "|      M|2000-01-01|2000-07-26|         208| 57.26793021946393| 95.09892939491029|\n",
      "|      L|2000-01-01|2000-07-26|         208| 55.02597445007184|107.26620838788999|\n",
      "|      D|2000-01-01|2000-07-26|         208| 60.31036891488962| 100.9251202118109|\n",
      "|      O|2000-01-01|2000-07-26|         208|49.859360275997446| 96.89470919142023|\n",
      "|      C|2000-01-01|2000-07-26|         208| 49.73101418317103| 97.52604699316623|\n",
      "|      B|2000-01-01|2000-07-26|         208|54.000718162666594| 97.51224357101201|\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/29 04:04:26 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@20668251 is aborting.\n",
      "23/04/29 04:04:26 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@20668251 aborted.\n",
      "23/04/29 04:04:26 WARN TaskSetManager: Lost task 4.0 in stage 20.0 (TID 164) (dafffbac9d94 executor driver): TaskKilled (Stage cancelled)\n",
      "23/04/29 04:04:26 WARN TaskSetManager: Lost task 5.0 in stage 20.0 (TID 165) (dafffbac9d94 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "counts_df = df.groupBy(\"station\").agg(\n",
    "    min(\"date\").alias(\"start\"),\n",
    "    max(\"date\").alias(\"end\"),\n",
    "    count(\"degrees\").alias(\"measurements\"),\n",
    "    avg(\"degrees\").alias(\"avg\"),\n",
    "    max(\"degrees\").alias(\"max\")\n",
    ")\n",
    "\n",
    "s = counts_df.writeStream.format(\"console\").trigger(processingTime=\"5 seconds\").outputMode(\"complete\").start()\n",
    "s.awaitTermination(30)\n",
    "s.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27daa824-82cc-4491-8b84-75d55bf7aae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/29 04:04:43 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_add\n",
    "\n",
    "today = df.select(\"station\", \"date\", (col(\"raining\").cast(\"int\")).alias(\"raining\"))\n",
    "yesterday = df.withColumn(\"date\", date_add(\"date\", -1)).select(\n",
    "    \"station\", \"date\", col(\"degrees\").alias(\"sub1degrees\"), col(\"raining\").cast(\"int\").alias(\"sub1raining\"))\n",
    "\n",
    "two_days_ago = df.withColumn(\"date\", date_add(\"date\", -2)).select(\n",
    "    \"station\", \"date\", col(\"degrees\").alias(\"sub2degrees\"), col(\"raining\").cast(\"int\").alias(\"sub2raining\"))\n",
    "\n",
    "features = yesterday.join(two_days_ago, [\"station\", \"date\"]).withColumn(\"month\", month(\"date\"))\n",
    "\n",
    "s2 = (\n",
    "    today\n",
    "    .join(features, [\"station\", \"date\"])\n",
    "    .repartition(1)\n",
    "    .writeStream\n",
    "    .format(\"parquet\")\n",
    "    .option(\"checkpointLocation\", \"checkpoint\")\n",
    "    .option(\"path\", \"parquet\")\n",
    "    .trigger(processingTime=\"1 minute\")\n",
    "    .start())\n",
    "\n",
    "s2.awaitTermination(61) #We set it to over a minute to assure at least one is created\n",
    "s2.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3352b514-6e18-4635-8352-8226c9ff2f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d0d399b-de46-44b3-9d24-24772650c338",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.parquet(\"parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1674c583-6139-4229-b425-005115846115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "511e512f-1906-4cc8-8d8a-290904554a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"month\", \"sub1degrees\", \"sub1raining\", \"sub2degrees\", \"sub2raining\"], outputCol=\"features\")\n",
    "data = assembler.transform(data)\n",
    "\n",
    "train_data, test_data = data.randomSplit([0.75, 0.25], seed=42)\n",
    "\n",
    "dt_classifier = DecisionTreeClassifier(labelCol=\"raining\", featuresCol=\"features\")\n",
    "dt_model = dt_classifier.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8014c53-00a8-416f-b3f8-b7da8c2cd0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_b579d834c827, depth=5, numNodes=31, numClasses=2, numFeatures=5\n",
      "  If (feature 2 <= 0.5)\n",
      "   If (feature 0 <= 2.5)\n",
      "    Predict: 0.0\n",
      "   Else (feature 0 > 2.5)\n",
      "    If (feature 0 <= 5.5)\n",
      "     If (feature 0 <= 3.5)\n",
      "      Predict: 0.0\n",
      "     Else (feature 0 > 3.5)\n",
      "      If (feature 3 <= 75.08611467875605)\n",
      "       Predict: 1.0\n",
      "      Else (feature 3 > 75.08611467875605)\n",
      "       Predict: 0.0\n",
      "    Else (feature 0 > 5.5)\n",
      "     If (feature 3 <= 45.84724427828422)\n",
      "      Predict: 1.0\n",
      "     Else (feature 3 > 45.84724427828422)\n",
      "      Predict: 0.0\n",
      "  Else (feature 2 > 0.5)\n",
      "   If (feature 0 <= 7.5)\n",
      "    If (feature 1 <= 61.190297186384086)\n",
      "     If (feature 0 <= 2.5)\n",
      "      If (feature 3 <= 34.55790222682037)\n",
      "       Predict: 1.0\n",
      "      Else (feature 3 > 34.55790222682037)\n",
      "       Predict: 0.0\n",
      "     Else (feature 0 > 2.5)\n",
      "      Predict: 1.0\n",
      "    Else (feature 1 > 61.190297186384086)\n",
      "     Predict: 1.0\n",
      "   Else (feature 0 > 7.5)\n",
      "    If (feature 4 <= 0.5)\n",
      "     If (feature 3 <= 86.44173951679099)\n",
      "      If (feature 1 <= 72.87046150541991)\n",
      "       Predict: 1.0\n",
      "      Else (feature 1 > 72.87046150541991)\n",
      "       Predict: 0.0\n",
      "     Else (feature 3 > 86.44173951679099)\n",
      "      Predict: 1.0\n",
      "    Else (feature 4 > 0.5)\n",
      "     If (feature 1 <= 89.57661792190575)\n",
      "      If (feature 0 <= 8.5)\n",
      "       Predict: 1.0\n",
      "      Else (feature 0 > 8.5)\n",
      "       Predict: 0.0\n",
      "     Else (feature 1 > 89.57661792190575)\n",
      "      Predict: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dt_model.toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e3bb59f-e6d4-49cd-9093-f93b842dbfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.7557077625570776\n",
      "Raining Avg: 0.3254421768707483\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "predictions = dt_model.transform(test_data)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"raining\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "rain_avg = data.agg(mean(col(\"raining\").cast(\"int\")).alias(\"raining\")).collect()[0][0]\n",
    "print(\"Model Accuracy:\", accuracy)\n",
    "print(\"Raining Avg:\", rain_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01d217b6-8289-4120-95b5-a9863e9df1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "def prediction_batching(predictions, batch_size=8, sleep_time=5):\n",
    "    total_rows = predictions.count()\n",
    "    num_batches = (total_rows + batch_size - 1) // batch_size\n",
    "    schema = predictions.select(\"station\", \"date\", \"prediction\").schema\n",
    "\n",
    "    for batch_num in range(3):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = __builtins__.min(start_index + batch_size, total_rows)\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Batch: {batch_num + 1}\")\n",
    "        print(\"-\" * 40)\n",
    "        rows = predictions.select(\"station\", \"date\", \"prediction\").take(end_index)[-batch_size:]\n",
    "        predictions_batch = predictions.sql_ctx.createDataFrame(rows, schema)\n",
    "        predictions_batch.show()\n",
    "        time.sleep(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be056116-979c-48e6-8085-f1f545c1a3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Batch: 1\n",
      "----------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|      date|prediction|\n",
      "+-------+----------+----------+\n",
      "|      A|2000-01-03|       0.0|\n",
      "|      A|2000-01-07|       0.0|\n",
      "|      A|2000-01-09|       0.0|\n",
      "|      A|2000-01-10|       0.0|\n",
      "|      A|2000-01-14|       0.0|\n",
      "|      A|2000-01-20|       1.0|\n",
      "|      A|2000-01-24|       0.0|\n",
      "|      A|2000-01-30|       0.0|\n",
      "+-------+----------+----------+\n",
      "\n",
      "----------------------------------------\n",
      "Batch: 2\n",
      "----------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|      date|prediction|\n",
      "+-------+----------+----------+\n",
      "|      A|2000-01-31|       0.0|\n",
      "|      A|2000-02-02|       0.0|\n",
      "|      A|2000-02-04|       1.0|\n",
      "|      A|2000-02-05|       1.0|\n",
      "|      A|2000-02-12|       0.0|\n",
      "|      A|2000-02-15|       0.0|\n",
      "|      A|2000-02-16|       0.0|\n",
      "|      A|2000-02-17|       0.0|\n",
      "+-------+----------+----------+\n",
      "\n",
      "----------------------------------------\n",
      "Batch: 3\n",
      "----------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|      date|prediction|\n",
      "+-------+----------+----------+\n",
      "|      A|2000-02-19|       0.0|\n",
      "|      A|2000-02-21|       0.0|\n",
      "|      A|2000-02-25|       0.0|\n",
      "|      A|2000-03-03|       1.0|\n",
      "|      A|2000-03-05|       0.0|\n",
      "|      A|2000-03-10|       1.0|\n",
      "|      A|2000-03-11|       1.0|\n",
      "|      A|2000-03-18|       1.0|\n",
      "+-------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.DataStreamWriter at 0x7f02838341f0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.writeStream.foreachBatch(prediction_batching(predictions, 8, 5)).trigger(processingTime=\"5 seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd882ed-9b8c-4d9c-9907-6d2dae0ef4aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
